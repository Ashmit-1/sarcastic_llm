{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from untrained_model import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 256,        # Embedding dimension\n",
    "    \"n_heads\": 4,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"sarcasm_finetuned_v1.pth\"))\n",
    "model.eval();\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
    "    \n",
    "### Instruction:\\n{entry['instruction']}\n",
    "    \"\"\"\n",
    "\n",
    "    return instruction_text\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from untrained_model import generate\n",
    "from untrained_model import text_to_token_ids\n",
    "from untrained_model import token_ids_to_text\n",
    "\n",
    "inference_device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(inference_device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n",
    "input_text = format_input({\"instruction\":\"Do you think politics is good ?\"})\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer).to(inference_device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=10,\n",
    "    temperature=0.7\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .replace(\"###\", \"\")\n",
    "    .replace(\"Response:\", \"\")\n",
    "    .replace(\"Response\", \"\")\n",
    "    .replace(\"<|endoftext|>\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
